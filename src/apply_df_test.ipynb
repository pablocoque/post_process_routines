{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadmodules import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import joblib\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_indices(counts):\n",
    "    return (np.arange(counts.sum()) - np.repeat(np.cumsum(counts) - counts, counts)).astype(np.int64)\n",
    "\n",
    "def mask_equal_to_previous(arr):\n",
    "    mask = np.ones(len(arr), dtype=bool)\n",
    "    mask[1:] = arr[1:] != arr[:-1]\n",
    "    return mask\n",
    "\n",
    "def B(x):\n",
    "    return sp.special.erf(x) * 2.*x*np.exp(-x**2)/np.sqrt(np.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './compare_comp_time/Au6_lvl5_31a2_eqrhnoevo/output/'\n",
    "\n",
    "num_snaps = 128\n",
    "files_per_snap = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the information of disrupted clusters to a log file\n",
    "with open('./output/disrupted_clusters_DF.txt', 'w') as log_file:\n",
    "    log_file.write(\"Snapshot, Time, PID, SCID, Initial Mass, Current Mass, FormationTime \\n\")\n",
    "\n",
    "for i in range(127, num_snaps):\n",
    "    print('Loading snapshot', i)\n",
    "    sf = load_subfind(i, dir=path, hdf5=True, loadonly=['fpos', 'frc2', 'svel', 'flty', 'fnsh', 'slty', 'spos', 'smty', 'ffsh'] )\n",
    "    s = gadget_readsnap(i, snappath=path, subfind=sf, hdf5=True, loadonlyhalo=0)\n",
    "    print('Redshift:', s.redshift, ' cosmo time:', s.time)\n",
    "\n",
    "    if ((s.data['type']==4).sum() > 0):\n",
    "        if((s.data['incl']>0).sum() > 0):\n",
    "            print('Found stars with GC in this snapshot')\n",
    "            s.calc_sf_indizes( sf )\n",
    "            s.select_halo( sf, use_principal_axis=True, use_cold_gas_spin=False, do_rotation=True, verbose=False )\n",
    "\n",
    "            Gcosmo = 43.\n",
    "            starparts = s.data['type']==4\n",
    "            \n",
    "            kinetic_energy = np.sum(s.data['vel']**2, axis=1)\n",
    "\n",
    "            orbital_energy = s.data['pot'] + 0.5 * kinetic_energy\n",
    "            Jtot = np.sqrt((np.cross( s.data['pos'], (s.data['vel'] ))**2).sum(axis=1))\n",
    "            \n",
    "            isort_parts = np.argsort(s.r())\n",
    "            revert_sort = np.argsort(isort_parts)\n",
    "            cummass = np.cumsum(s.data['mass'][isort_parts])\n",
    "            Vc_parts = np.sqrt(Gcosmo*cummass[revert_sort]/s.r())\n",
    "\n",
    "            # Energy of circular orbits at increasing radii\n",
    "            Ecirc = 0.5*Vc_parts[isort_parts]**2 + s.data['pot'][isort_parts]\n",
    "            e_max = np.nanmax(Ecirc[~np.isinf(Ecirc)])\n",
    "            orbital_energy -= e_max\n",
    "            Ecirc -= e_max\n",
    "\n",
    "            mask = mask_equal_to_previous(s.r()[isort_parts][~np.isinf(Ecirc)])\n",
    "\n",
    "            r_test = np.logspace(-5., np.log10(s.r().max()), 500)\n",
    "            Ecirc_f = sp.interpolate.PchipInterpolator(s.r()[isort_parts][~np.isinf(Ecirc)][mask], Ecirc[~np.isinf(Ecirc)][mask])\n",
    "            Vc_f = sp.interpolate.PchipInterpolator(s.r()[isort_parts][~np.isinf(Ecirc)][mask], Vc_parts[isort_parts][~np.isinf(Ecirc)][mask])\n",
    "            Mr_f = sp.interpolate.PchipInterpolator(s.r()[isort_parts][~np.isinf(Ecirc)][mask], cummass[~np.isinf(Ecirc)][mask])\n",
    "    \n",
    "            mask_clusters_initial = (s.data['incl'] > 0)\n",
    "    \n",
    "            idx = np.argmin(np.abs(orbital_energy[starparts][mask_clusters_initial,np.newaxis] - Ecirc_f(r_test)), axis=1)\n",
    "            rc = r_test[idx]\n",
    "            vc = Vc_f(rc)\n",
    "            Lzmax = rc*vc\n",
    "            \n",
    "            cluster_masses = s.data['mclt'][mask_clusters_initial].flatten()\n",
    "            init_cluster_masses = s.data['imcl'][mask_clusters_initial].flatten()\n",
    "            cluster_mlost_sh = s.data['mlsk'][mask_clusters_initial].flatten()\n",
    "            cluster_mlost_rx = s.data['mlrx'][mask_clusters_initial].flatten()\n",
    "            not_empty_clusters = (init_cluster_masses > 0.)\n",
    "            cluster_masses = cluster_masses[not_empty_clusters]\n",
    "            cluster_mlost_sh = cluster_mlost_sh[not_empty_clusters]\n",
    "            cluster_mlost_rx = cluster_mlost_rx[not_empty_clusters]\n",
    "            init_cluster_masses = init_cluster_masses[not_empty_clusters]\n",
    "\n",
    "            part_id = np.repeat(s.data['id'][starparts], s.data['incl'])\n",
    "            scs_id = expand_indices(s.data['incl'][mask_clusters_initial])\n",
    "            \n",
    "            clusters_formtime = np.repeat(s.data['age'], s.data['incl'])\n",
    "            clusters_age = s.cosmology_get_lookback_time_from_a(clusters_formtime, is_flat=True) - s.cosmology_get_lookback_time_from_a(s.time, is_flat=True)\n",
    "            \n",
    "            # Do the DF timescale estimate for clusters with mass\n",
    "            mask_mass = (cluster_masses > 0.)\n",
    "    \n",
    "            rc_clus = np.repeat(rc, s.data['incl'][mask_clusters_initial])\n",
    "            M_rc_clus = np.repeat(Mr_f(rc), s.data['incl'][mask_clusters_initial])\n",
    "            vc_rc_clus = np.repeat(vc, s.data['incl'][mask_clusters_initial])\n",
    "            sigma_rc_clus = np.zeros_like(rc_clus)\n",
    "\n",
    "            joblib.dump(s.r(), 'parts_radius.npy')\n",
    "            joblib.dump(starparts, 'starparts.npy')\n",
    "            joblib.dump(s.data['age'], 's_data_age.npy')\n",
    "            joblib.dump(s.data['vel'], 's_data_vel.npy')  # assuming 3D velocity\n",
    "            joblib.dump(s.data['type'], 's_data_type.npy')\n",
    "\n",
    "            parts_radius = joblib.load('parts_radius.npy', mmap_mode='r')\n",
    "            s_data_age = joblib.load('s_data_age.npy', mmap_mode='r')\n",
    "            s_data_vel = joblib.load('s_data_vel.npy', mmap_mode='r')\n",
    "            s_data_type = joblib.load('s_data_type.npy', mmap_mode='r')\n",
    "            starparts = joblib.load('starparts.npy', mmap_mode='r')\n",
    "\n",
    "            def velocity_dispersion(radius, parts_radius, starparts, s_data_age, s_data_vel, s_data_type):\n",
    "                within_radius = parts_radius[starparts][s_data_age > 0.] < radius\n",
    "                if within_radius.sum() >= 48:\n",
    "                    velocities = np.sqrt(np.sum(s_data_vel[starparts][s_data_age > 0.][within_radius]**2, axis=1))\n",
    "                else:\n",
    "                    mask_dm = (s_data_type != 4) * (s_data_type != 0)\n",
    "                    within_radius = parts_radius[mask_dm] < radius\n",
    "                    velocities = np.sqrt(np.sum(s_data_vel[mask_dm][within_radius]**2, axis=1))\n",
    "\n",
    "                return np.std(velocities) if velocities.size else 0.0\n",
    "\n",
    "            sigma_clus = Parallel(n_jobs=-1)(delayed(velocity_dispersion)(r, parts_radius, starparts, s_data_age, s_data_vel, s_data_type)\n",
    "                                              for r in rc[s.data['nclt'][mask_clusters_initial]>0])\n",
    "            sigma_rc_clus[mask_mass] = np.repeat(sigma_clus, s.data['nclt'][mask_clusters_initial][s.data['nclt'][mask_clusters_initial]>0])\n",
    "\n",
    "            os.remove('parts_radius.npy')\n",
    "            os.remove('starparts.npy')\n",
    "            os.remove('s_data_age.npy')\n",
    "            os.remove('s_data_vel.npy')\n",
    "            os.remove('s_data_type.npy')\n",
    "\n",
    "            feps = np.repeat((Jtot[starparts][mask_clusters_initial]/Lzmax)**0.78, s.data['incl'][mask_clusters_initial])\n",
    "            coulumblog = np.zeros_like(rc_clus)\n",
    "            coulumblog[mask_mass] = np.log(1. + M_rc_clus[mask_mass]/cluster_masses[mask_mass])\n",
    "            \n",
    "            tdf = 2e4 * np.ones_like(rc_clus)\n",
    "            tdf[mask_mass] = feps[mask_mass]/(2*B(vc_rc_clus[mask_mass]/(np.sqrt(2.)*sigma_rc_clus[mask_mass])))*np.sqrt(2.)*sigma_rc_clus[mask_mass]* \\\n",
    "                            rc_clus[mask_mass]**2./(Gcosmo*cluster_masses[mask_mass]*coulumblog[mask_mass])\n",
    "            tdf *= s.UnitLength_in_cm/s.UnitVelocity_in_cm_per_s / (1e9*365.25*24*3600)\n",
    "\n",
    "            mask_disrupted = (tdf < clusters_age)\n",
    "\n",
    "            # Write the information of disrupted clusters to a log file\n",
    "            with open('./output/disrupted_clusters_DF.txt', 'a') as log_file:\n",
    "                for idx in np.where(mask_disrupted)[0]:\n",
    "                    log_file.write(f\"{i}, {s.time}, {part_id[idx]}, {scs_id[idx]}, {init_cluster_masses[idx]}, {cluster_masses[idx]}, {clusters_formtime[idx]}\\n\")\n",
    "\n",
    "            # Now go through all subsequent snapshots and remove the disrupted clusters\n",
    "            if (mask_disrupted.sum() > 0):\n",
    "                print('Clusters disrupted by dynamical friction {:d}'.format(mask_disrupted.sum()))\n",
    "                for j in range(i, num_snaps):\n",
    "                    found = 0\n",
    "                    k = 0\n",
    "                    while found < mask_disrupted.sum():\n",
    "                        h5_file = h5py.File(path + 'snapdir_{:03d}/snapshot_{:03d}.{:d}.hdf5'.format(j,j,k), 'r+')\n",
    "                        stars = h5_file['PartType4']\n",
    "                        ids = stars['ParticleIDs'][:]\n",
    "                        clus_mass = stars['ClusterMass'][:]\n",
    "                        clus_radius = stars['ClusterRadius'][:]\n",
    "                        disruption_time = stars['DisruptionTime'][:]\n",
    "                        mlost_shocks = stars['MassLostShocks'][:]\n",
    "                        mlost_relax = stars['MassLostRelaxation'][:]\n",
    "                        nclus = stars['NumberOfClusters'][:]\n",
    "                        inverse_mask = np.isin(part_id[mask_disrupted], ids)\n",
    "                        found += inverse_mask.sum()\n",
    "                        if inverse_mask.sum()>0:\n",
    "                            print('Found {:d} clusters in snapshot {:d} part {:d}'.format(inverse_mask.sum(), j, k))\n",
    "                            for cl_idx in range(inverse_mask.sum()):\n",
    "                                mask_id = np.isin(ids, part_id[mask_disrupted][inverse_mask][cl_idx])\n",
    "                                clus_mass[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = 0.0\n",
    "                                clus_radius[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = 0.0\n",
    "                                mlost_shocks[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = cluster_mlost_sh[mask_disrupted][inverse_mask][cl_idx]\n",
    "                                mlost_relax[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = cluster_mlost_rx[mask_disrupted][inverse_mask][cl_idx]\n",
    "                                nclus[mask_id] = (clus_mass[mask_id] > 0.).sum()\n",
    "                                disruption_time[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = s.time\n",
    "                        stars['ClusterMass'][:] = clus_mass\n",
    "                        stars['ClusterRadius'][:] = clus_radius\n",
    "                        stars['DisruptionTime'][:] = disruption_time\n",
    "                        stars['MassLostShocks'][:] = mlost_shocks\n",
    "                        stars['MassLostRelaxation'][:] = mlost_relax\n",
    "                        stars['NumberOfClusters'][:] = nclus\n",
    "                        h5_file.close()\n",
    "                        k+=1\n",
    "            else:\n",
    "                print('No clusters disrupted by dynamical friction in this snapshot')\n",
    "        else:\n",
    "            print('NO STARS WITH GC IN MAIN HALO IN THIS SNAPSHOT')\n",
    "    else:\n",
    "        print('No stars in this snapshot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify SCs that have info that need to be corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# disrupted_clusters_DF.txt content example:\n",
    "# Snapshot:0, Time:1, PID:2, SCID:3, Initial Mass:4, Current Mass:5, FormationTime:6\n",
    "disrupted_scs = [\n",
    "[62, 0.344341999449342, 8796098036440, 0, 6.574573490070179e-05, 3.3196440199390054e-05, 0.31109389662742615],\n",
    "[62, 0.344341999449342, 8796098085888, 0, 5.590692671830766e-05, 3.454939724178985e-05, 0.30888494849205017],\n",
    "[62, 0.344341999449342, 8796098581042, 0, 0.00021077124984003603, 0.00013148513971827924, 0.3019043505191803],\n",
    "[62, 0.344341999449342, 8796098774989, 0, 1.1844956134154927e-05, 5.524972038983833e-06, 0.3049369752407074],\n",
    "[62, 0.344341999449342, 8796090239295, 0, 0.0007212080527096987, 0.00043532243580557406, 0.23272006213665009]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation = 'Au6_lvl4_cfea_sh50myr10/output/'\n",
    "num_snaps = 128\n",
    "files_per_snap = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cluster 1 in snapshot 62 part 0\n",
      "[0.] [0.] [7.5806724e-06] [6.267579e-07] [0] [0.344342]\n",
      "Found cluster 1 in snapshot 127 part 0\n",
      "Need to fix cluster 1 in snapshot 127 part 0\n",
      "[0.] [0.] [6.1110795e-06] [3.7020984e-06] [0] [0.344342]\n",
      "[0.] [0.] [7.5806724e-06] [6.267579e-07] [0] [0.344342]\n",
      "Found cluster 1 in snapshot 126 part 0\n",
      "From this snapshot down the cluster should be correct\n",
      "Found cluster 2 in snapshot 62 part 0\n",
      "[0.] [0.] [1.3543137e-06] [9.462226e-07] [0] [0.344342]\n",
      "Found cluster 2 in snapshot 127 part 0\n",
      "Need to fix cluster 2 in snapshot 127 part 0\n",
      "[0.] [0.] [6.1110795e-06] [3.7020984e-06] [0] [0.344342]\n",
      "[0.] [0.] [1.3543137e-06] [9.462226e-07] [0] [0.344342]\n",
      "Found cluster 2 in snapshot 126 part 0\n",
      "From this snapshot down the cluster should be correct\n",
      "Found cluster 3 in snapshot 62 part 0\n",
      "[0.] [0.] [6.277888e-06] [1.3067355e-06] [0] [0.344342]\n",
      "Found cluster 3 in snapshot 127 part 0\n",
      "Need to fix cluster 3 in snapshot 127 part 0\n",
      "[0.] [0.] [6.1110795e-06] [3.7020984e-06] [0] [0.344342]\n",
      "[0.] [0.] [6.277888e-06] [1.3067355e-06] [0] [0.344342]\n",
      "Found cluster 3 in snapshot 126 part 0\n",
      "From this snapshot down the cluster should be correct\n",
      "Found cluster 4 in snapshot 62 part 0\n",
      "[0.] [0.] [1.2267113e-06] [6.9883856e-07] [0] [0.344342]\n",
      "Found cluster 4 in snapshot 127 part 0\n",
      "Need to fix cluster 4 in snapshot 127 part 0\n",
      "[0.] [0.] [6.1110795e-06] [3.7020984e-06] [0] [0.344342]\n",
      "[0.] [0.] [1.2267113e-06] [6.9883856e-07] [0] [0.344342]\n",
      "Found cluster 4 in snapshot 126 part 0\n",
      "From this snapshot down the cluster should be correct\n",
      "Found cluster 5 in snapshot 62 part 0\n",
      "[0.] [0.] [6.1110795e-06] [3.7020984e-06] [0] [0.344342]\n",
      "Found cluster 5 in snapshot 127 part 0\n",
      "From this snapshot down the cluster should be correct\n"
     ]
    }
   ],
   "source": [
    "for sc in range(len(disrupted_scs)):\n",
    "    for j in range(files_per_snap):\n",
    "        h5_file = h5py.File(simulation + 'snapdir_{:03d}/snapshot_{:03d}.{:d}.hdf5'.format(disrupted_scs[sc][0],disrupted_scs[sc][0],j), 'r')\n",
    "        stars = h5_file['PartType4']\n",
    "        ids = stars['ParticleIDs'][:]\n",
    "        clus_mass = stars['ClusterMass'][:]\n",
    "        clus_radius = stars['ClusterRadius'][:]\n",
    "        disruption_time = stars['DisruptionTime'][:]\n",
    "        mlost_shocks = stars['MassLostShocks'][:]\n",
    "        mlost_relax = stars['MassLostRelaxation'][:]\n",
    "        nclus = stars['NumberOfClusters'][:]\n",
    "        cl_idx = int(disrupted_scs[sc][3])\n",
    "        inverse_mask = np.isin(ids, disrupted_scs[sc][2])\n",
    "        if inverse_mask.sum()>0:\n",
    "            print('Found cluster {:d} in snapshot {:d} part {:d}'.format(sc+1, disrupted_scs[sc][0], j))\n",
    "            print(clus_mass[inverse_mask, cl_idx],\n",
    "                  clus_radius[inverse_mask, cl_idx],\n",
    "                  mlost_shocks[inverse_mask, cl_idx],\n",
    "                  mlost_relax[inverse_mask, cl_idx],\n",
    "                  nclus[inverse_mask],\n",
    "                  disruption_time[inverse_mask, cl_idx])\n",
    "            snap = num_snaps - 1\n",
    "            need_to_correct = True\n",
    "            while need_to_correct:\n",
    "                for part in range(files_per_snap):\n",
    "                    h5_file_next = h5py.File(simulation + 'snapdir_{:03d}/snapshot_{:03d}.{:d}.hdf5'.format(snap,snap,part), 'r+')\n",
    "                    stars_next = h5_file_next['PartType4']\n",
    "                    ids_next = stars_next['ParticleIDs'][:]\n",
    "                    clus_mass_next = stars_next['ClusterMass'][:]\n",
    "                    clus_radius_next = stars_next['ClusterRadius'][:]\n",
    "                    disruption_time_next = stars_next['DisruptionTime'][:]\n",
    "                    mlost_shocks_next = stars_next['MassLostShocks'][:]\n",
    "                    mlost_relax_next = stars_next['MassLostRelaxation'][:]\n",
    "                    nclus_next = stars_next['NumberOfClusters'][:]\n",
    "                    inverse_mask_next = np.isin(ids_next, disrupted_scs[sc][2])\n",
    "                    if inverse_mask_next.sum()>0:\n",
    "                        print('Found cluster {:d} in snapshot {:d} part {:d}'.format(sc+1, snap, part))\n",
    "                        if(clus_mass_next[inverse_mask_next, cl_idx] > 0. or disruption_time_next[inverse_mask_next, cl_idx] != disrupted_scs[sc][1]\n",
    "                           or mlost_shocks_next[inverse_mask_next, cl_idx] != mlost_shocks[inverse_mask, cl_idx]\n",
    "                           or mlost_relax_next[inverse_mask_next, cl_idx] != mlost_relax[inverse_mask, cl_idx]):\n",
    "                            print('Need to fix cluster {:d} in snapshot {:d} part {:d}'.format(sc+1, snap, part))\n",
    "                            print(clus_mass_next[inverse_mask_next, cl_idx],\n",
    "                                  clus_radius_next[inverse_mask_next, cl_idx],\n",
    "                                  mlost_shocks_next[inverse_mask_next, cl_idx],\n",
    "                                  mlost_relax_next[inverse_mask_next, cl_idx],\n",
    "                                  nclus_next[inverse_mask_next],\n",
    "                                  disruption_time_next[inverse_mask_next, cl_idx])\n",
    "                            clus_mass_next[inverse_mask_next, cl_idx] = 0.\n",
    "                            clus_radius_next[inverse_mask_next, cl_idx] = 0.\n",
    "                            mlost_shocks_next[inverse_mask_next, cl_idx] = mlost_shocks[inverse_mask, cl_idx]\n",
    "                            mlost_relax_next[inverse_mask_next, cl_idx] = mlost_relax[inverse_mask, cl_idx]\n",
    "                            nclus_next[inverse_mask_next] = (clus_mass_next[inverse_mask_next] > 0.).sum()\n",
    "                            disruption_time_next[inverse_mask_next, cl_idx] = disrupted_scs[sc][1]\n",
    "                            print(clus_mass_next[inverse_mask_next, cl_idx],\n",
    "                                  clus_radius_next[inverse_mask_next, cl_idx],\n",
    "                                  mlost_shocks_next[inverse_mask_next, cl_idx],\n",
    "                                  mlost_relax_next[inverse_mask_next, cl_idx],\n",
    "                                  nclus_next[inverse_mask_next],\n",
    "                                  disruption_time_next[inverse_mask_next, cl_idx])\n",
    "                            stars_next['ClusterMass'][:] = clus_mass_next\n",
    "                            stars_next['ClusterRadius'][:] = clus_radius_next\n",
    "                            stars_next['DisruptionTime'][:] = disruption_time_next\n",
    "                            stars_next['MassLostShocks'][:] = mlost_shocks_next\n",
    "                            stars_next['MassLostRelaxation'][:] = mlost_relax_next\n",
    "                            stars_next['NumberOfClusters'][:] = nclus_next\n",
    "                            snap -= 1\n",
    "                        else:\n",
    "                            print('From this snapshot down the cluster should be correct')\n",
    "                            need_to_correct = False\n",
    "                        part = files_per_snap  # break the loop\n",
    "                    h5_file_next.close()\n",
    "            j = files_per_snap  # break the loop\n",
    "        h5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
