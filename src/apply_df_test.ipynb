{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loadmodules import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "from numba import njit, config\n",
    "config.THREADING_LAYER = 'omp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_indices(counts):\n",
    "    return (np.arange(counts.sum()) - np.repeat(np.cumsum(counts) - counts, counts)).astype(np.int64)\n",
    "\n",
    "def mask_equal_to_previous(arr):\n",
    "    mask = np.ones(len(arr), dtype=bool)\n",
    "    mask[1:] = arr[1:] != arr[:-1]\n",
    "    return mask\n",
    "\n",
    "def B(x):\n",
    "    return sp.special.erf(x) * 2.*x*np.exp(-x**2)/np.sqrt(np.pi)\n",
    "\n",
    "@njit(parallel=True)\n",
    "def velocity_dispersion(radius, parts_radius, starparts, s_data_age, s_data_vel, s_data_type):\n",
    "    # Select particles within the given radius\n",
    "    within_radius = parts_radius[starparts][s_data_age > 0.] < radius\n",
    "    if within_radius.sum() >= 48:\n",
    "        velocities = np.sqrt(np.sum(s_data_vel[starparts][s_data_age > 0.][within_radius]**2, axis=1))\n",
    "    # if not enough stars, use dark matter particles\n",
    "    else:\n",
    "        mask_dm = (s_data_type != 4) * (s_data_type != 0)\n",
    "        within_radius = parts_radius[mask_dm] < radius\n",
    "        velocities = np.sqrt(np.sum(s_data_vel[mask_dm][within_radius]**2, axis=1))\n",
    "\n",
    "    # Calculate the velocity dispersion\n",
    "    if velocities.size == 0:\n",
    "        return 0.0\n",
    "    # Avoid division by zero\n",
    "    else:\n",
    "        return np.std(velocities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './Au6_lvl4_d8ad_testchanges/output/'\n",
    "\n",
    "num_snaps = 128\n",
    "files_per_snap = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading snapshot 127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/aripcont/pylib/gadget_snap.py:732: H5pyDeprecationWarning: Using astype() as a context manager is deprecated. Slice the returned object instead, like: ds.astype(np.int32)[:10]\n",
      "  with dset.astype('uint64'):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Redshift: 2.220446049250313e-16  cosmo time: 0.9999999999999998\n",
      "Found stars with GC in this snapshot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters disrupted by dynamical friction 1\n",
      "Found 1 clusters in snapshot 127 part 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(127, num_snaps):\n",
    "    print('Loading snapshot', i)\n",
    "    sf = load_subfind(i, dir=path, hdf5=True, loadonly=['fpos', 'frc2', 'svel', 'flty', 'fnsh', 'slty', 'spos', 'smty', 'ffsh'] )\n",
    "    s = gadget_readsnap(i, snappath=path, subfind=sf, hdf5=True, loadonlyhalo=0)\n",
    "    print('Redshift:', s.redshift, ' cosmo time:', s.time)\n",
    "\n",
    "    if ((s.data['type']==4).sum() > 0):\n",
    "        if((s.data['incl']>0).sum() > 0):\n",
    "            print('Found stars with GC in this snapshot')\n",
    "            s.calc_sf_indizes( sf )\n",
    "            galrad = 0.1 * sf.data['frc2'][0]\n",
    "            s.select_halo( sf, use_principal_axis=True, use_cold_gas_spin=False, do_rotation=True, verbose=False )\n",
    "\n",
    "            Gcosmo = 43.\n",
    "            starparts = s.data['type']==4\n",
    "            \n",
    "            kinetic_energy = np.sum(s.data['vel']**2, axis=1)\n",
    "\n",
    "            orbital_energy = s.data['pot'] + 0.5 * kinetic_energy\n",
    "            orbital_energy /= 1e5\n",
    "\n",
    "            e_max = orbital_energy.max()\n",
    "            orbital_energy -= e_max\n",
    "\n",
    "            Jtot = np.sqrt((np.cross( s.data['pos'], (s.data['vel'] ))**2).sum(axis=1))\n",
    "            Lz = np.cross( s.data['pos'], (s.data['vel'] ) )[:,0]\n",
    "            Lz *= np.sign(np.nanmedian(Lz))\n",
    "\n",
    "            isort_parts = np.argsort(s.r())\n",
    "            revert_sort = np.argsort(isort_parts)\n",
    "            cummass = np.cumsum(s.data['mass'][isort_parts])\n",
    "            Vc_parts = np.sqrt(Gcosmo*cummass[revert_sort]/s.r())\n",
    "\n",
    "            # Energy of circular orbits at increasing radii\n",
    "            Ecirc = 0.5*Vc_parts[isort_parts]**2 + s.data['pot'][isort_parts]\n",
    "            Ecirc /= 1e5\n",
    "            Ecirc -= e_max\n",
    "\n",
    "            mask = mask_equal_to_previous(s.r()[isort_parts][~np.isinf(Ecirc)])\n",
    "\n",
    "            r_test = np.logspace(-4.5, np.log10(s.r().max()), 500)\n",
    "            Ecirc_f = sp.interpolate.PchipInterpolator(s.r()[isort_parts][~np.isinf(Ecirc)][mask], Ecirc[~np.isinf(Ecirc)][mask])\n",
    "            Vc_f = sp.interpolate.PchipInterpolator(s.r()[isort_parts][~np.isinf(Ecirc)][mask], Vc_parts[isort_parts][~np.isinf(Ecirc)][mask])\n",
    "            Mr_f = sp.interpolate.PchipInterpolator(s.r()[isort_parts][~np.isinf(Ecirc)][mask], cummass[~np.isinf(Ecirc)][mask])\n",
    "    \n",
    "            mask_clusters_initial = (s.data['incl'] > 0)\n",
    "    \n",
    "            idx = np.argmin(np.abs(orbital_energy[starparts][mask_clusters_initial,np.newaxis] - Ecirc_f(r_test)), axis=1)\n",
    "            rc = r_test[idx]\n",
    "            vc = Vc_f(rc)\n",
    "            Lzmax = rc*vc\n",
    "            circ_param = Lz[starparts][mask_clusters_initial]/Lzmax\n",
    "    \n",
    "            cluster_masses = s.data['mclt'][mask_clusters_initial].flatten()\n",
    "            init_cluster_masses = s.data['imcl'][mask_clusters_initial].flatten()\n",
    "            cluster_mlost_sh = s.data['mlsk'][mask_clusters_initial].flatten()\n",
    "            cluster_mlost_rx = s.data['mlrx'][mask_clusters_initial].flatten()\n",
    "            not_empty_clusters = (init_cluster_masses > 0.)\n",
    "            cluster_masses = cluster_masses[not_empty_clusters]\n",
    "            cluster_mlost_sh = cluster_mlost_sh[not_empty_clusters]\n",
    "            cluster_mlost_rx = cluster_mlost_rx[not_empty_clusters]\n",
    "            init_cluster_masses = init_cluster_masses[not_empty_clusters]\n",
    "\n",
    "            part_id = np.repeat(s.data['id'][starparts], s.data['incl'])\n",
    "            scs_id = expand_indices(s.data['incl'][mask_clusters_initial])\n",
    "            \n",
    "            clusters_formtime = np.repeat(s.data['age'], s.data['incl'])\n",
    "            clusters_age = s.cosmology_get_lookback_time_from_a(clusters_formtime, is_flat=True) - s.cosmology_get_lookback_time_from_a(s.time, is_flat=True)\n",
    "            # do the DF timescale estimate for clusters with mass\n",
    "            mask_mass = (cluster_masses > 0.)\n",
    "    \n",
    "            rc_clus = np.repeat(rc, s.data['incl'][mask_clusters_initial])\n",
    "            M_rc_clus = np.repeat(Mr_f(rc), s.data['incl'][mask_clusters_initial])\n",
    "            vc_rc_clus = np.repeat(vc, s.data['incl'][mask_clusters_initial])\n",
    "            sigma_rc_clus = np.zeros_like(rc_clus)\n",
    "            sigma_clus = np.array([velocity_dispersion(r, s.r(), starparts, s.data['age'], s.data['vel'], s.data['type']) for r in \n",
    "                                    rc[s.data['nclt'][mask_clusters_initial]>0]])\n",
    "            sigma_rc_clus[mask_mass] = np.repeat(sigma_clus, s.data['nclt'][mask_clusters_initial][s.data['nclt'][mask_clusters_initial]>0])\n",
    "\n",
    "            feps = np.repeat((Jtot[starparts][mask_clusters_initial]/Lzmax)**0.78, s.data['incl'][mask_clusters_initial])\n",
    "            coulumblog = np.zeros_like(rc_clus)\n",
    "            coulumblog[mask_mass] = np.log(1. + M_rc_clus[mask_mass]/cluster_masses[mask_mass])\n",
    "            \n",
    "            tdf = 2e4 * np.ones_like(rc_clus)\n",
    "            tdf[mask_mass] = feps[mask_mass]/(2*B(vc_rc_clus[mask_mass]/(np.sqrt(2.)*sigma_rc_clus[mask_mass])))*np.sqrt(2.)*sigma_rc_clus[mask_mass]* \\\n",
    "                            rc_clus[mask_mass]**2./(Gcosmo*cluster_masses[mask_mass]*coulumblog[mask_mass])\n",
    "            tdf *= s.UnitLength_in_cm/s.UnitVelocity_in_cm_per_s / (1e9*365.25*24*3600)\n",
    "\n",
    "            mask_disrupted = (tdf < clusters_age)\n",
    "            if (mask_disrupted.sum() > 0):\n",
    "                print('Clusters disrupted by dynamical friction {:d}'.format(mask_disrupted.sum()))\n",
    "                for j in range(i, num_snaps):\n",
    "                    found = 0\n",
    "                    k = 0\n",
    "                    while found < mask_disrupted.sum():\n",
    "                        h5_file = h5py.File(path + 'snapdir_{:03d}/snapshot_{:03d}.{:d}.hdf5'.format(j,j,k), 'r+')\n",
    "                        stars = h5_file['PartType4']\n",
    "                        ids = stars['ParticleIDs'][:]\n",
    "                        clus_mass = stars['ClusterMass'][:]\n",
    "                        clus_radius = stars['ClusterRadius'][:]\n",
    "                        disruption_time = stars['DisruptionTime'][:]\n",
    "                        mlost_shocks = stars['MassLostShocks'][:]\n",
    "                        mlost_relax = stars['MassLostRelaxation'][:]\n",
    "                        nclus = stars['NumberOfClusters'][:]\n",
    "                        inverse_mask = np.isin(part_id[mask_disrupted], ids)\n",
    "                        found += inverse_mask.sum()\n",
    "                        if inverse_mask.sum()>0:\n",
    "                            print('Found {:d} clusters in snapshot {:d} part {:d}'.format(inverse_mask.sum(), j, k))\n",
    "                            for cl_idx in range(inverse_mask.sum()):\n",
    "                                mask_id = np.isin(ids, part_id[mask_disrupted][inverse_mask][cl_idx])\n",
    "                                clus_mass[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = 0.0\n",
    "                                clus_radius[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = 0.0\n",
    "                                mlost_shocks[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = cluster_mlost_sh[mask_disrupted][inverse_mask][cl_idx]\n",
    "                                mlost_relax[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = cluster_mlost_rx[mask_disrupted][inverse_mask][cl_idx]\n",
    "                                nclus[mask_id] = (clus_mass[mask_id] > 0.).sum()\n",
    "                                disruption_time[mask_id, scs_id[mask_disrupted][inverse_mask][cl_idx]] = s.time\n",
    "                        stars['ClusterMass'][:] = clus_mass\n",
    "                        stars['ClusterRadius'][:] = clus_radius\n",
    "                        stars['DisruptionTime'][:] = disruption_time\n",
    "                        stars['MassLostShocks'][:] = mlost_shocks\n",
    "                        stars['MassLostRelaxation'][:] = mlost_relax\n",
    "                        stars['NumberOfClusters'][:] = nclus\n",
    "                        h5_file.close()\n",
    "                        k+=1\n",
    "            else:\n",
    "                print('No clusters disrupted by dynamical friction in this snapshot')\n",
    "        else:\n",
    "            print('NO STARS WITH GC IN MAIN HALO IN THIS SNAPSHOT')\n",
    "    else:\n",
    "        print('No stars in this snapshot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
